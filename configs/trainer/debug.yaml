defaults:
  - default.yaml

min_epochs: 1
max_epochs: 1
# prints
profiler: null

gpus: 1

resume_from_checkpoint: null

check_val_every_n_epoch: 50

# debugs
# This flag runs a “unit test” by running n if set to n (int) else 1 if set to True training and validation batch(es).
# The point is to detect any bugs in the training/validation loop without having to wait for a full epoch to crash.
fast_dev_run: 0

# Sometimes it’s helpful to only use a percentage of your training, val or test data (or a set number of batches).
# For example, you can use 20% of the training set and 1% of the validation set.
limit_train_batches: 1.0
limit_val_batches: 1.0
limit_test_batches: 1.0

track_grad_norm: -1

# Enable anomaly detection for the autograd engine.
detect_anomaly: false

# A good debugging technique is to take a tiny portion of your data (say 2 samples per class), and try to get your model
# to overfit. If it can’t, it’s a sign it won’t work with large datasets.
overfit_batches: 0.0

# Lightning runs a few steps of validation in the beginning of training. This avoids crashing in the validation
# loop sometime deep into a lengthy training loop.
num_sanity_val_steps: 0

